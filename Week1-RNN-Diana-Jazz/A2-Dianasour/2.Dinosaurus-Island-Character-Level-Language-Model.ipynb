{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1a96eb8-e6d7-4760-8c7c-288f1fe2fc34",
   "metadata": {},
   "source": [
    "# Character level language model - Dinosaurus Island\n",
    "Welcome to Dinosaurus Island! 65 million years ago, dinosaurs existed, and in this assignment, they have returned.\n",
    "\n",
    "You are in charge of a special task: Leading biology researchers are creating new breeds of dinosaurs and bringing them to life on earth, and your job is to give names to these dinosaurs. If a dinosaur does not like its name, it might go berserk, so choose wisely!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae498e2e-9f22-47ea-a8c5-93eaa0cf1b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import *\n",
    "import random\n",
    "import pprint\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bfe3015-a0a5-4d79-91ff-d754edf0240e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19909 total characters and 27 unique characters in your data.\n"
     ]
    }
   ],
   "source": [
    "data = open('dinos.txt', 'r').read()\n",
    "data= data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83089d6-e306-4a3b-bcfb-6016761de219",
   "metadata": {},
   "source": [
    "- The characters are a-z (26 characters) plus the \"\\n\" (or newline character).\n",
    "- In this assignment, the newline character \"\\n\" plays a role similar to the $<EOS>$ (or \"End of sentence\") token discussed in lecture.\n",
    "    - Here, \"\\n\" indicates the end of the dinosaur name rather than the end of a sentence.\n",
    "- __char_to_ix:__ In the cell below, you'll create a Python dictionary (i.e., a hash table) to map each character to an index from 0-26.\n",
    "- __ix_to_char:__ Then, you'll create a second Python dictionary that maps each index back to the corresponding character.\n",
    "    - This will help you figure out which index corresponds to which character in the probability distribution output of the softmax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9db66055-b939-4fed-8a3b-1fe7e0f3a939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(chars)\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fca02fe-a4c7-4be0-99b7-d6c4a2c97d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   0: '\\n',\n",
      "    1: 'a',\n",
      "    2: 'b',\n",
      "    3: 'c',\n",
      "    4: 'd',\n",
      "    5: 'e',\n",
      "    6: 'f',\n",
      "    7: 'g',\n",
      "    8: 'h',\n",
      "    9: 'i',\n",
      "    10: 'j',\n",
      "    11: 'k',\n",
      "    12: 'l',\n",
      "    13: 'm',\n",
      "    14: 'n',\n",
      "    15: 'o',\n",
      "    16: 'p',\n",
      "    17: 'q',\n",
      "    18: 'r',\n",
      "    19: 's',\n",
      "    20: 't',\n",
      "    21: 'u',\n",
      "    22: 'v',\n",
      "    23: 'w',\n",
      "    24: 'x',\n",
      "    25: 'y',\n",
      "    26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(ix_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aa1574-4702-4da7-b251-f847fb9e804d",
   "metadata": {},
   "source": [
    "## 1.2 - Overview of the Model\n",
    "Your model will have the following structure:\n",
    "\n",
    "- Initialize parameters\n",
    "- Run the optimization loop\n",
    "    - Forward propagation to compute the loss function\n",
    "    - Backward propagation to compute the gradients with respect to the loss function\n",
    "    - Clip the gradients to avoid exploding gradients\n",
    "    - Using the gradients, update your parameters with the gradient descent update rule.\n",
    "- Return the learned parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7a4fa1-73ce-45da-9b07-78fbd1ab5e0e",
   "metadata": {},
   "source": [
    "## 2 - Building Blocks of the Model\n",
    "In this part, you will build two important blocks of the overall model:\n",
    "\n",
    "1. Gradient clipping: to avoid exploding gradients\n",
    "2. Sampling: a technique used to generate characters\n",
    "\n",
    "You will then apply these two functions to build the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816ad96f-ee0c-4125-8ded-b6dae19c2114",
   "metadata": {},
   "source": [
    "## Exercise 1 - clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "892e52d8-eb16-4d9f-8c4d-eb8a9fa74774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(gradients, maxValue):\n",
    "    gradients = copy.deepcopy(gradients)\n",
    "\n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "\n",
    "    for gradient in gradients:\n",
    "        np.clip(gradients[gradient], -maxValue, maxValue, out = gradients[gradient])\n",
    "\n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5030682b-c4ea-42d3-b60c-9da2b8a63b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradients for mValue=10\n",
      "gradients[\"dWaa\"][1][2] = 10.0\n",
      "gradients[\"dWax\"][3][1] = -10.0\n",
      "gradients[\"dWya\"][1][2] = 0.2971381536101662\n",
      "gradients[\"db\"][4] = [10.]\n",
      "gradients[\"dby\"][1] = [8.45833407]\n",
      "\u001b[92mAll tests passed!\u001b[0m\n",
      "\n",
      "Gradients for mValue=5\n",
      "gradients[\"dWaa\"][1][2] = 5.0\n",
      "gradients[\"dWax\"][3][1] = -5.0\n",
      "gradients[\"dWya\"][1][2] = 0.2971381536101662\n",
      "gradients[\"db\"][4] = [5.]\n",
      "gradients[\"dby\"][1] = [5.]\n",
      "\u001b[92mAll tests passed!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Test with a max value of 10\n",
    "def clip_test(target, mValue):\n",
    "    print(f\"\\nGradients for mValue={mValue}\")\n",
    "    np.random.seed(3)\n",
    "    dWax = np.random.randn(5, 3) * 10\n",
    "    dWaa = np.random.randn(5, 5) * 10\n",
    "    dWya = np.random.randn(2, 5) * 10\n",
    "    db = np.random.randn(5, 1) * 10\n",
    "    dby = np.random.randn(2, 1) * 10\n",
    "    gradients = {\"dWax\": dWax, \"dWaa\": dWaa, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "\n",
    "    gradients2 = target(gradients, mValue)\n",
    "    print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients2[\"dWaa\"][1][2])\n",
    "    print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients2[\"dWax\"][3][1])\n",
    "    print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients2[\"dWya\"][1][2])\n",
    "    print(\"gradients[\\\"db\\\"][4] =\", gradients2[\"db\"][4])\n",
    "    print(\"gradients[\\\"dby\\\"][1] =\", gradients2[\"dby\"][1])\n",
    "    \n",
    "    for grad in gradients2.keys():\n",
    "        valuei = gradients[grad]\n",
    "        valuef = gradients2[grad]\n",
    "        mink = np.min(valuef)\n",
    "        maxk = np.max(valuef)\n",
    "        assert mink >= -abs(mValue), f\"Problem with {grad}. Set a_min to -mValue in the np.clip call\"\n",
    "        assert maxk <= abs(mValue), f\"Problem with {grad}.Set a_max to mValue in the np.clip call\"\n",
    "        index_not_clipped = np.logical_and(valuei <= mValue, valuei >= -mValue)\n",
    "        assert np.all(valuei[index_not_clipped] == valuef[index_not_clipped]), f\" Problem with {grad}. Some values that should not have changed, changed during the clipping process.\"\n",
    "    \n",
    "    print(\"\\033[92mAll tests passed!\\x1b[0m\")\n",
    "    \n",
    "clip_test(clip, 10)\n",
    "clip_test(clip, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c363a9-f729-42e4-951f-e107843dc810",
   "metadata": {},
   "source": [
    "## 2.2 - Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3496d227-b175-46b0-b32b-7205b8bd88a3",
   "metadata": {},
   "source": [
    "- __Step 1:__ Input the \"dummy\" vector of zeros $x^{<1>} = {0}^{->}$.\n",
    "    - This is the default input before you've generated any characters. You also set \n",
    "- __Step 2:__ Run one step of forward propagation to get $a^{<1>}$ and $\\hat {y} ^{<1>}$. Here are the equations:\n",
    "hidden state:\n",
    "\n",
    "- $a^{<t+1>} = tanh(W_{ax} \\; x^{<t+1>} \\; + W_{aa} \\; a^{<t>} + b $ --> (1)\n",
    "- $z^{<t+1>} = W_{ya} \\; a^{<t+1>} \\; + b_y $ --> (2)\n",
    "- $\\hat {y} ^{<t+1>} = softmax(z^{<t+1>})$ --> (3)\n",
    "\n",
    "Note that $\\hat {y} ^{<t+1>} $ is a (softmax) probability vector (its entries are between 0 and 1 and sum to 1).\n",
    "\n",
    "$\\hat {y}_i ^{<t+1>} $ represents the probability that the character indexed by \"i\" is the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf8ab749-160f-49ae-b856-8e0071d0543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(parameters, char_to_ix, seed):\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "    print(\"Value of n_a is: \", n_a)\n",
    "\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "\n",
    "    indices = []\n",
    "    idx = -1\n",
    "\n",
    "    counter = 0\n",
    "    newline_character = char_to_ix['\\n']\n",
    "\n",
    "    while (idx !=newline_character and counter !=50):\n",
    "        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) +b)\n",
    "        z = np.dot(Wya, a) + by\n",
    "        y = softmax(z)\n",
    "\n",
    "        np.random.seed(counter + seed)\n",
    "        idx = np.random.choice(range(len(y)), p = np.squeeze(y))\n",
    "    \n",
    "        indices.append(idx)\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[idx] = 1\n",
    "        a_prev = a\n",
    "        seed += 1\n",
    "        counter += 1\n",
    "\n",
    "    if(counter ==50):\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "    return indices\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b70953b-b1fe-4064-a4f4-20a12b69eae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of n_a is:  100\n",
      "Sampling:\n",
      "list of sampled indices: \n",
      " [23, 16, 26, 26, 24, 3, 21, 1, 7, 24, 15, 3, 25, 20, 6, 13, 10, 8, 20, 12, 2, 0]\n",
      "list of sampled characters: \n",
      " ['w', 'p', 'z', 'z', 'x', 'c', 'u', 'a', 'g', 'x', 'o', 'c', 'y', 't', 'f', 'm', 'j', 'h', 't', 'l', 'b', '\\n']\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "def sample_test(target):\n",
    "    np.random.seed(24)\n",
    "    _, n_a = 20, 100\n",
    "    Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
    "    b , by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
    "    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
    "\n",
    "    indices = target(parameters, char_to_ix, 0)\n",
    "    print(\"Sampling:\")\n",
    "    print(\"list of sampled indices: \\n\", indices)\n",
    "    print(\"list of sampled characters: \\n\", [ix_to_char[i] for i in indices])\n",
    "    \n",
    "    assert len(indices) < 52, \"Indices length must be smaller than 52\"\n",
    "    assert indices[-1] == char_to_ix['\\n'], \"All samples must end with \\\\n\"\n",
    "    assert min(indices) >= 0 and max(indices) < len(char_to_ix), f\"Sampled indexes must be between 0 and len(char_to_ix)={len(char_to_ix)}\"\n",
    "    assert np.allclose(indices[0:6], [23, 16, 26, 26, 24, 3]), \"Wrong values\"\n",
    "\n",
    "    print(\"\\033[92mAll tests passed!\")\n",
    "sample_test(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1e7b64e-d8f7-47e4-bb46-c201c3bbd00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
    "    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n",
    "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
    "    gradients = clip(gradients, 5)\n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "    return loss, gradients, a[len(X)-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c01db62-5a15-4564-808c-3a729df5e5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 126.50397572165369\n",
      "gradients[\"dWaa\"][1][2] = 0.19470931534723046\n",
      "np.argmax(gradients[\"dWax\"]) = 93\n",
      "gradients[\"dWya\"][1][2] = -0.007773876032004632\n",
      "gradients[\"db\"][4] = [-0.06809825]\n",
      "gradients[\"dby\"][1] = [0.01538192]\n",
      "a_last[4] = [-1.]\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "def optimize_test(target):\n",
    "    np.random.seed(1)\n",
    "    vocab_size, n_a = 27, 100\n",
    "    a_prev = np.random.randn(n_a, 1)\n",
    "    Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
    "    b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
    "    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
    "    X = [12, 3, 5, 11, 22, 3]\n",
    "    Y = [4, 14, 11, 22, 25, 26]\n",
    "\n",
    "    old_parameters = copy.deepcopy(parameters)\n",
    "    loss, gradients, a_last = target(X, Y, a_prev, parameters, learning_rate = 0.01)\n",
    "    print(\"Loss =\", loss)\n",
    "    print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "    print(\"np.argmax(gradients[\\\"dWax\\\"]) =\", np.argmax(gradients[\"dWax\"]))\n",
    "    print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
    "    print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
    "    print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])\n",
    "    print(\"a_last[4] =\", a_last[4])\n",
    "\n",
    "    assert np.isclose(loss, 126.5039757), \"Problems with the call of the rnn_forward function\"\n",
    "    for grad in gradients.values():\n",
    "        assert np.min(grad) >= -5, \"Problems in the clip function call\"\n",
    "        assert np.max(grad) <= 5, \"Problems in the clip function call\"\n",
    "    assert np.allclose(gradients['dWaa'][1, 2], 0.1947093), \"Unexpected gradients, Check the rnn_backward call\"\n",
    "    assert np.allclose(gradients['dWya'][1, 2], -0.007773876), \"Unexpected gradients. Check the rnn_backward call\"\n",
    "    assert not np.allclose(parameters['Wya'], old_parameters['Wya']), \"parameters were not updated\"\n",
    "\n",
    "    print(\"\\033[92mAll tests passed!\")\n",
    "optimize_test(optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a1830ad-0a33-41f1-9e56-e00773a102f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(data_x, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 27, verbose = False):\n",
    "    \"\"\"\n",
    "    Trains the model and generates dinosaur names. \n",
    "    \n",
    "    Arguments:\n",
    "    data_x -- text corpus, divided in words\n",
    "    ix_to_char -- dictionary that maps the index to a character\n",
    "    char_to_ix -- dictionary that maps a character to an index\n",
    "    num_iterations -- number of iterations to train the model for\n",
    "    n_a -- number of units of the RNN cell\n",
    "    dino_names -- number of dinosaur names you want to sample at each iteration. \n",
    "    vocab_size -- number of unique characters found in the text (size of the vocabulary)\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- learned parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve n_x and n_y from vocab_size\n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "    \n",
    "    # Initialize loss (this is required because we want to smooth our loss)\n",
    "    loss = get_initial_loss(vocab_size, dino_names)\n",
    "    \n",
    "    # Build list of all dinosaur names (training examples).\n",
    "    examples = [x.strip() for x in data_x]\n",
    "    \n",
    "    # Shuffle list of all dinosaur names\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(examples)\n",
    "    \n",
    "    # Initialize the hidden state of your LSTM\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # for grading purposes\n",
    "    last_dino_name = \"abc\"\n",
    "    \n",
    "    # Optimization loop\n",
    "    for j in range(num_iterations):\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        # Set the index `idx` (see instructions above)\n",
    "        idx = j%len(examples)\n",
    "        \n",
    "        # Set the input X (see instructions above)\n",
    "        single_example_chars = examples[idx]\n",
    "        single_example_ix = [char_to_ix[c] for c in single_example_chars]\n",
    "\n",
    "        # if X[t] == None, we just have x[t]=0. This is used to set the input for the first timestep to the zero vector. \n",
    "        X = [None] + single_example_ix\n",
    "        \n",
    "        # Set the labels Y (see instructions above)\n",
    "        # The goal is to train the RNN to predict the next letter in the name\n",
    "        # So the labels are the list of characters that are one time-step ahead of the characters in the input X\n",
    "        Y = X[1:] \n",
    "        # The RNN should predict a newline at the last letter, so add ix_newline to the end of the labels\n",
    "        ix_newline = [char_to_ix[\"\\n\"]]\n",
    "        Y = Y + ix_newline\n",
    "\n",
    "        # Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters\n",
    "        # Choose a learning rate of 0.01\n",
    "        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # debug statements to aid in correctly forming X, Y\n",
    "        if verbose and j in [0, len(examples) -1, len(examples)]:\n",
    "            print(\"j = \" , j, \"idx = \", idx,) \n",
    "        if verbose and j in [0]:\n",
    "            #print(\"single_example =\", single_example)\n",
    "            print(\"single_example_chars\", single_example_chars)\n",
    "            print(\"single_example_ix\", single_example_ix)\n",
    "            print(\" X = \", X, \"\\n\", \"Y =       \", Y, \"\\n\")\n",
    "        \n",
    "        # Use a latency trick to keep the loss smooth. It happens here to accelerate the training.\n",
    "        loss = smooth(loss, curr_loss)\n",
    "\n",
    "        # Every 1000 Iteration, generate \"n\" characters thanks to sample() to check if the model is learning properly\n",
    "        if j % 1000 == 0:\n",
    "            \n",
    "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "            \n",
    "            # The number of dinosaur names to print\n",
    "            seed = 0\n",
    "            for name in range(dino_names):\n",
    "                \n",
    "                # Sample indices and print them\n",
    "                sampled_indices = sample(parameters, char_to_ix, seed)\n",
    "                last_dino_name = get_sample(sampled_indices, ix_to_char)\n",
    "                print(last_dino_name.replace('\\n', ''))\n",
    "                \n",
    "                seed += 1  # To get the same result (for grading purposes), increment the seed by one. \n",
    "      \n",
    "            print('\\n')\n",
    "        \n",
    "    return parameters, last_dino_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca035a76-d763-4334-92dc-cadf571b579c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j =  0 idx =  0\n",
      "single_example_chars turiasaurus\n",
      "single_example_ix [20, 21, 18, 9, 1, 19, 1, 21, 18, 21, 19]\n",
      " X =  [None, 20, 21, 18, 9, 1, 19, 1, 21, 18, 21, 19] \n",
      " Y =        [20, 21, 18, 9, 1, 19, 1, 21, 18, 21, 19, 0] \n",
      "\n",
      "Iteration: 0, Loss: 23.087336\n",
      "\n",
      "Nkzxwtdmfqoeyhsqwasjkjvu\n",
      "Kneb\n",
      "Kzxwtdmfqoeyhsqwasjkjvu\n",
      "Neb\n",
      "Zxwtdmfqoeyhsqwasjkjvu\n",
      "Eb\n",
      "Xwtdmfqoeyhsqwasjkjvu\n",
      "\n",
      "\n",
      "Iteration: 1000, Loss: 28.712699\n",
      "\n",
      "Nivusahidoraveros\n",
      "Ioia\n",
      "Iwtroeoirtaurusabrngeseaosawgeanaitafeaolaeratohop\n",
      "Nac\n",
      "Xtroeoirtaurusabrngeseaosawgeanaitafeaolaeratohopr\n",
      "Ca\n",
      "Tseeohnnaveros\n",
      "\n",
      "\n",
      "j =  1535 idx =  1535\n",
      "j =  1536 idx =  0\n",
      "Iteration: 2000, Loss: 27.884160\n",
      "\n",
      "Liusskeomnolxeros\n",
      "Hmdaairus\n",
      "Hytroligoraurus\n",
      "Lecalosapaus\n",
      "Xusicikoraurus\n",
      "Abalpsamantisaurus\n",
      "Tpraneronxeros\n",
      "\n",
      "\n",
      "Iteration: 3000, Loss: 26.863598\n",
      "\n",
      "Niusos\n",
      "Infa\n",
      "Iusrtendor\n",
      "Nda\n",
      "Wtrololos\n",
      "Ca\n",
      "Tps\n",
      "\n",
      "\n",
      "Iteration: 4000, Loss: 25.901815\n",
      "\n",
      "Mivrosaurus\n",
      "Inee\n",
      "Ivtroplisaurus\n",
      "Mbaaisaurus\n",
      "Wusichisaurus\n",
      "Cabaselachus\n",
      "Toraperlethosdarenitochusthiamamumamaon\n",
      "\n",
      "\n",
      "Iteration: 5000, Loss: 25.290275\n",
      "\n",
      "Ngyusedonis\n",
      "Klecagropechus\n",
      "Lytosaurus\n",
      "Necagropechusangotmeeycerum\n",
      "Xuskangosaurus\n",
      "Da\n",
      "Tosaurus\n",
      "\n",
      "\n",
      "Iteration: 6000, Loss: 24.608779\n",
      "\n",
      "Onwusceomosaurus\n",
      "Lieeaerosaurus\n",
      "Lxussaurus\n",
      "Oma\n",
      "Xusteonosaurus\n",
      "Eeahosaurus\n",
      "Toreonosaurus\n",
      "\n",
      "\n",
      "Iteration: 7000, Loss: 24.425330\n",
      "\n",
      "Ngytromiasaurus\n",
      "Ingabcosaurus\n",
      "Kyusichiropurusanrasauraptous\n",
      "Necamithachusidinysaus\n",
      "Yusodon\n",
      "Caaesaurus\n",
      "Tosaurus\n",
      "\n",
      "\n",
      "Iteration: 8000, Loss: 24.070350\n",
      "\n",
      "Onxusichepriuon\n",
      "Kilabersaurus\n",
      "Lutrodon\n",
      "Omaaerosaurus\n",
      "Xutrcheps\n",
      "Edaksoje\n",
      "Trodiktonus\n",
      "\n",
      "\n",
      "Iteration: 9000, Loss: 23.730944\n",
      "\n",
      "Onyusaurus\n",
      "Klecanotal\n",
      "Kyuspang\n",
      "Ogaacosaurus\n",
      "Xutrasaurus\n",
      "Dabcosaurus\n",
      "Troching\n",
      "\n",
      "\n",
      "Iteration: 10000, Loss: 23.844446\n",
      "\n",
      "Onyusaurus\n",
      "Klecalosaurus\n",
      "Lustodon\n",
      "Ola\n",
      "Xusodonia\n",
      "Eeaeosaurus\n",
      "Troceosaurus\n",
      "\n",
      "\n",
      "Iteration: 11000, Loss: 23.581901\n",
      "\n",
      "Leutosaurus\n",
      "Inda\n",
      "Itrtoplerosherotarangos\n",
      "Lecalosaurus\n",
      "Xutogolosaurus\n",
      "Babator\n",
      "Trodonosaurus\n",
      "\n",
      "\n",
      "Iteration: 12000, Loss: 23.291971\n",
      "\n",
      "Onyxosaurus\n",
      "Kica\n",
      "Lustrepiosaurus\n",
      "Olaagrraiansaurus\n",
      "Yuspangosaurus\n",
      "Eealosaurus\n",
      "Trognesaurus\n",
      "\n",
      "\n",
      "Iteration: 13000, Loss: 23.547611\n",
      "\n",
      "Nixrosaurus\n",
      "Indabcosaurus\n",
      "Jystolong\n",
      "Necalosaurus\n",
      "Yuspangosaurus\n",
      "Daagosaurus\n",
      "Usndicirax\n",
      "\n",
      "\n",
      "Iteration: 14000, Loss: 23.382339\n",
      "\n",
      "Meutromodromurus\n",
      "Inda\n",
      "Iutroinatorsaurus\n",
      "Maca\n",
      "Yusteratoptititan\n",
      "Ca\n",
      "Troclosaurus\n",
      "\n",
      "\n",
      "Iteration: 15000, Loss: 23.059559\n",
      "\n",
      "Phyusichinnaumus\n",
      "Lidaaeropa\n",
      "Lustrapiorax\n",
      "Padaeronbanthus\n",
      "Yusrapilomujusiasaurus\n",
      "Edalosaurus\n",
      "Trodonicgyinos\n",
      "\n",
      "\n",
      "Iteration: 16000, Loss: 23.246280\n",
      "\n",
      "Meustolophodus\n",
      "Inda\n",
      "Iusspandosaurus\n",
      "Maca\n",
      "Yusocops\n",
      "Daaerosaurus\n",
      "Trrarin\n",
      "\n",
      "\n",
      "Iteration: 17000, Loss: 23.159572\n",
      "\n",
      "Onyusgfiankhugshuaschismaosaurus\n",
      "Kiadanrus\n",
      "Lutromia\n",
      "Olaadrus\n",
      "Yustarasonthritan\n",
      "Eealosaurus\n",
      "Trojmciomurus\n",
      "\n",
      "\n",
      "Iteration: 18000, Loss: 22.862995\n",
      "\n",
      "Physsrhia\n",
      "Melbahosaurus\n",
      "Myssphonosaurus\n",
      "Pegamosaurus\n",
      "Ytronia\n",
      "Eg\n",
      "Tromacosaurus\n",
      "\n",
      "\n",
      "Iteration: 19000, Loss: 23.056035\n",
      "\n",
      "Onustriongonus\n",
      "Kigaagstacastes\n",
      "Lussliorosaurus\n",
      "Ola\n",
      "Yuspangsaurus\n",
      "Eg\n",
      "Trtdoipritasaurus\n",
      "\n",
      "\n",
      "Iteration: 20000, Loss: 22.955209\n",
      "\n",
      "Onyxrongchusapros\n",
      "Logcalosaurus\n",
      "Lustrionnoptitho\n",
      "Olaa\n",
      "Yusmangosaurus\n",
      "Ehagosaurus\n",
      "Trsarimelverluclongtochuoven\n",
      "\n",
      "\n",
      "Iteration: 21000, Loss: 22.823275\n",
      "\n",
      "Oryusaurus\n",
      "Koeia\n",
      "Lustodon\n",
      "Ola\n",
      "Yusocherosscorocorantodosaurus\n",
      "Daaeros\n",
      "Trokomis\n",
      "\n",
      "\n",
      "Iteration: 22000, Loss: 22.845740\n",
      "\n",
      "Onyushanesaurus\n",
      "Llecaipthabosaurus\n",
      "Mwrrodon\n",
      "Ola\n",
      "Yusocheosaurus\n",
      "Eg\n",
      "Trodon\n",
      "\n",
      "\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "parameters, last_name = model(data.split(\"\\n\"), ix_to_char, char_to_ix, 22001, verbose = True)\n",
    "\n",
    "print(\"\\033[92mAll tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956cb9da-5f96-43d9-a09f-c4aef0c6de2a",
   "metadata": {},
   "source": [
    "## Shakespear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7e14bd10-155d-4ec5-ba59-cab371a411a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text data...\n",
      "Creating training set...\n",
      "number of training examples: 31412\n",
      "Vectorizing training set...\n",
      "Loading model...\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Input, Masking\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.utils import get_file\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from shakespeare_utils import *\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "efa54c73-a1f3-4a90-8ff6-acf8c030a186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 62s 251ms/step - loss: 2.5620\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e28a9bd970>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y, batch_size=128, epochs=1, callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1eebc1f4-796c-41d8-af06-71d71787ca61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Write the beginning of your poem, the Shakespeare machine will complete it. Your input is:  Forsooth this maketh no sense\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Here is your poem: \n",
      "\n",
      "Forsooth this maketh no sense,\n",
      "and men'g for the edtint high tandle gend.\n",
      "our i fro go gacet which lider,\n",
      "thre vaanso your when sor, kist, com to so say by weved\n",
      "the galnten my mistian) looks hath with year.\n",
      "frecun ti pruve prome in my tuitans her),\n",
      "to how hath (al she wrome i show find for thee.\n",
      "ting prifceroos dall cosns to with every.\n",
      "sur yer, the geasules bromsu and oll not,\n",
      "that shanr gevery hers fury knows o hack,\n",
      "thoug"
     ]
    }
   ],
   "source": [
    "# Run this cell to try with different inputs without having to re-train the model \n",
    "generate_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5224264c-a4e0-47f4-99e5-06abf70f37bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
